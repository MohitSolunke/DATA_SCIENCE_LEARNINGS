{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaGh7I4tGy30jWckj2LxlM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohitSolunke/DATA_SCIENCE_LEARNINGS/blob/main/NLP_Bag_of_Words_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbgn3PgILyJN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "nJkN8rsZL6Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = \"!@ He love % NLP !\"\n",
        "\n",
        "import re # Regular Expression or regex\n"
      ],
      "metadata": {
        "id": "wBmS97cML9KN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# How to use Regex"
      ],
      "metadata": {
        "id": "wFIZsOk7MR0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sent2 = \"It is raining outside\"\n",
        "print(re.sub(\"raining\",\"sunny\", sent2))\n",
        "\n",
        "sent3 = \"Thank you very very much\"\n",
        "print(re.sub(\"very\",\"so\", sent3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkqmxjPgMVW-",
        "outputId": "26e39023-f5a7-43b3-d618-5fbebf5fa48b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is sunny outside\n",
            "Thank you so so much\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = \"!@ He love % NLP !\"\n",
        "print(re.sub(\"[^a-zA-Z]\",\" \", sent1))\n",
        "#alternate\n",
        "#print(re.sub(\"[!@%]\",\" \",sent1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAVI04w6MqlS",
        "outputId": "4fd89ad1-4c1e-422e-8b52-a0614f01c458"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   He love   NLP  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph  = \"\"\"Creating a single paragraph of 200,000 words isn't feasible in this format. However, I can provide a detailed, lengthy text about sentiment analysis in NLP, and you can then expand upon it as needed. Hereâ€™s a comprehensive overview:\n",
        "\n",
        "---\n",
        "\n",
        "Sentiment analysis, also known as opinion mining, is a subfield of Natural Language Processing (NLP) that focuses on identifying and extracting subjective information from text. It involves determining the sentiment expressed in a piece of text, which can be positive, negative, or neutral. The applications of sentiment analysis are vast and varied, spanning multiple domains such as market research, customer feedback, social media monitoring, and more. In essence, sentiment analysis helps organizations understand the emotions and opinions of their audience, enabling them to make data-driven decisions. The process typically involves several steps: data collection, text preprocessing, feature extraction, sentiment classification, and evaluation. Data collection involves gathering text data from various sources like social media, reviews, and surveys. Text preprocessing includes cleaning the text by removing stop words, punctuation, and performing tokenization, stemming, or lemmatization. Feature extraction involves converting the text into numerical features that can be fed into a machine learning model. Common techniques include Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and word embeddings like Word2Vec or GloVe. Sentiment classification is then performed using machine learning algorithms such as Naive Bayes, Support Vector Machines (SVM), or deep learning models like Recurrent Neural Networks (RNN) and Transformer-based models (e.g., BERT). Finally, the model's performance is evaluated using metrics like accuracy, precision, recall, and F1-score. The challenges in sentiment analysis include handling sarcasm, idiomatic expressions, context dependency, and domain-specific language. Additionally, multilingual sentiment analysis adds another layer of complexity due to the diversity of languages and cultural nuances. Advanced techniques like transfer learning and ensemble methods are being explored to address these challenges. The future of sentiment analysis looks promising with ongoing research in more sophisticated models and techniques, particularly in the realm of deep learning and reinforcement learning. As organizations continue to recognize the value of understanding sentiment in their data, the demand for accurate and efficient sentiment analysis solutions will only grow, driving further innovation in the field.\n",
        "\n",
        "---\n",
        "\n",
        "Feel free to expand on any section to reach your desired length.\"\"\""
      ],
      "metadata": {
        "id": "ZC4YRzcnPfFI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## If You are doing the sentiment analysis then you can use these tech"
      ],
      "metadata": {
        "id": "UF4aivqTNIjS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "zAhVOcl6TNJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = []\n",
        "for i in range (len(sentences)):\n",
        "  new_sent = re.sub(\"[^a-zA-Z]\",\" \",sentences[i]) # data cleaning\n",
        "  new_sent = new_sent.lower() # Lowering th all words\n",
        "  new_sent = new_sent.split() # Splitting the words\n",
        "  # To remove stop words\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  new_sent = [ps.stem(word) for word in new_sent if not word in set(nltk.corpus.stopwords.words('english'))]\n",
        "  new_sent = \" \".join(new_sent)\n",
        "  corpus.append(new_sent)\n",
        "\n",
        "  ## Data is ready\n",
        "  ## Convert Text to number\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zsshGCrOvqZ",
        "outputId": "8219169a-e027-4342-febb-53b44c4423fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UyDalwtO_gR",
        "outputId": "a5f541f9-9685-4269-cc43-ab876f9f11c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['creat singl paragraph word feasibl format', 'howev provid detail lengthi text sentiment analysi nlp expand upon need', 'comprehens overview sentiment analysi also known opinion mine subfield natur languag process nlp focus identifi extract subject inform text', 'involv determin sentiment express piec text posit neg neutral', 'applic sentiment analysi vast vari span multipl domain market research custom feedback social media monitor', 'essenc sentiment analysi help organ understand emot opinion audienc enabl make data driven decis', 'process typic involv sever step data collect text preprocess featur extract sentiment classif evalu', 'data collect involv gather text data variou sourc like social media review survey', 'text preprocess includ clean text remov stop word punctuat perform token stem lemmat', 'featur extract involv convert text numer featur fed machin learn model', 'common techniqu includ bag word bow term frequenc invers document frequenc tf idf word embed like word vec glove', 'sentiment classif perform use machin learn algorithm naiv bay support vector machin svm deep learn model like recurr neural network rnn transform base model e g bert', 'final model perform evalu use metric like accuraci precis recal f score', 'challeng sentiment analysi includ handl sarcasm idiomat express context depend domain specif languag', 'addit multilingu sentiment analysi add anoth layer complex due divers languag cultur nuanc', 'advanc techniqu like transfer learn ensembl method explor address challeng', 'futur sentiment analysi look promis ongo research sophist model techniqu particularli realm deep learn reinforc learn', 'organ continu recogn valu understand sentiment data demand accur effici sentiment analysi solut grow drive innov field', 'feel free expand section reach desir length']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_ehOw_N7TY3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features= 2000, stop_words='english', lowercase= True)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkP5Kp1WSxy1",
        "outputId": "9e3c7c39-5028-417b-c364-b0ba7084895f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pudabSgcTpKS",
        "outputId": "9e9b96cb-084b-4afa-9d8b-181eff05064b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DisAdvantage\n",
        " a) Semantic Info is no capture\n",
        " b) Context not\n",
        " c) Weightage is not\n",
        " d)Bigger dataset is not work well\n",
        " e) sparsity\n",
        "\n",
        "## Adavantage\n",
        "a) Easy to implement\n",
        "b) processing time less\n",
        "c) Work well with small dataset"
      ],
      "metadata": {
        "id": "kvPgfg4TU1uN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l4rjXTqwT-QY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}