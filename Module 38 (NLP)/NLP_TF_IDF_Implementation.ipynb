{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77UW63YtVW3D",
        "outputId": "4624c877-908d-40db-a4b3-21b795667eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'and', 'bag', 'by', 'case', 'corpus', 'demonstrate', 'few', 'first', 'followed', 'in', 'is', 'more', 'of', 'one', 'our', 'second', 'sentence', 'the', 'third', 'this', 'title', 'to', 'upper', 'with', 'words']\n",
            "There are 26 words in vocabulary.\n",
            "A total of 44 words in corpus.\n"
          ]
        }
      ],
      "source": [
        "# How to find unique word in your corpus\n",
        "# Vocab\n",
        "# List type Corpus\n",
        "corpus = ['This is the first sentence in our corpus followed by one more sentence to demonstrate bag of words',\n",
        "          'THIS is the second sentence in our corpus with a FEW UPPER WORDS and few Title case words ',\n",
        "          'This is the third sentence in our corpus']\n",
        "\n",
        "vocab = []\n",
        "total_words = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "  sentence = sentence.lower()                 ## Going inside the each sentence of corpus\n",
        "  token_temp = sentence.split()               ## Spliting the each sentence in words\n",
        "  total_words = total_words + len(token_temp) ## measure the total words\n",
        "  for i in range (len(token_temp)):           ## Going to each word\n",
        "    if token_temp[i] not in vocab:            ## check the word is present in vocab or not if not\n",
        "      vocab.append(token_temp[i])             ## Then append it in vocab\n",
        "\n",
        "vocab.sort()                                  ## Sort the vocab\n",
        "print(vocab)\n",
        "print('There are {} words in vocabulary.'.format(len(vocab)))\n",
        "print('A total of {} words in corpus.'.format(total_words))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF - IDF (Term Frequency - Inverse Doccument Frequency )\n",
        "\n",
        "* The main difference in BOW and TF-IDF is that it is give Weightage to term and BOW is not give Weightage to words"
      ],
      "metadata": {
        "id": "R7h85hHycuCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "data = ['I love Natural Language processing',\n",
        "        ' Creating word vectors',\n",
        "        'Is my jam!']\n",
        "#fit\n",
        "tfvectorizer = TfidfVectorizer()\n",
        "vectorized_data= tfvectorizer.fit_transform(data)\n",
        "vectorized_data\n",
        "vectorized_data.toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLDwzXx3Y2mC",
        "outputId": "ecc72efc-b2db-452e-ae2c-6475930f0a77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.5       , 0.5       ,\n",
              "        0.        , 0.5       , 0.5       , 0.        , 0.        ],\n",
              "       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.57735027, 0.57735027],\n",
              "       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n",
              "        0.57735027, 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(data)\n",
        "X.toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHqtulLHjcpa",
        "outputId": "94151ef7-1869-4ba0-936d-ce94995d8186"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 1, 1, 0, 0, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## You can see the difference between the BOW and TF-IDF by theire respective result\n",
        "\n",
        "## Advantage of TF-IDF\n",
        "\n",
        "#1) It weightage the words => Importance of total_words\n",
        "#2) It is applicable for big data\n",
        "#3) sensitive to corpus data size\n",
        "#4) Biased to rare term\n",
        "\n",
        "\n",
        "## DisAdvantage of TF-IDF\n",
        "\n",
        "#1) Lossing the comntext information is missing\n",
        "#2) It dosent matter order of Words or order of words is missing\n"
      ],
      "metadata": {
        "id": "EXTibBgTjtGJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3jW_Vb_Fj3Ix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}