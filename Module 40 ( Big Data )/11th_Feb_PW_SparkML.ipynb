{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIYSNebBgnmB",
        "outputId": "51f2c718-9f67-4e46-c804-077d5d26bd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=d608c5093bd98bc87ac9486a9ac97b35f93984e50ed586331e923f2077a6bf73\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8rh0D5HPh5SR"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lqkMy6oFiHxB"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('crew_requirement').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "X-gBPlf4iZr5",
        "outputId": "81b46d3f-51c9-4a77-9a16-380a37f92fd0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/cruise_ship_info.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c191be61cd0a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/cruise_ship_info.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/cruise_ship_info.csv."
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('/content/cruise_ship_info.csv', inferSchema = True, header = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-wdTXVwi9av"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGPv_gLRjIhC"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u3WRc3Vj5lY"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'Cruise_line', outputCol = \"Cruise_line_index\")\n",
        "\n",
        "df_indexed = indexer.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU5NyDjIn1Qq"
      },
      "outputs": [],
      "source": [
        "df_indexed.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP7NwWi7oB42"
      },
      "outputs": [],
      "source": [
        "df_indexed.select('Cruise_line', 'Cruise_line_index').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TxcbDKEoxJ8"
      },
      "outputs": [],
      "source": [
        "df_indexed.groupby(\"Cruise_line\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1fFLmiHpbr1"
      },
      "outputs": [],
      "source": [
        "df_indexed.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jHPwiFipGLH"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols = ['Age',\n",
        " 'Tonnage',\n",
        " 'passengers',\n",
        " 'length',\n",
        " 'cabins',\n",
        " 'passenger_density',\n",
        " 'Cruise_line_index'], outputCol = 'features')\n",
        "\n",
        "output = assembler.transform(df_indexed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHKjPu-opv5K"
      },
      "outputs": [],
      "source": [
        "output.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ALQSbZgqJ9s"
      },
      "outputs": [],
      "source": [
        "train_data , test_data = output.randomSplit([.8,.2])\n",
        "train_data.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uctYHX0rBXz"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "crew_req = LinearRegression(featuresCol= 'features', labelCol = 'crew')\n",
        "\n",
        "model = crew_req.fit(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8du67UcrfgN"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUGTwV9frmWY"
      },
      "outputs": [],
      "source": [
        "result.r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDO2jPM1rz6I"
      },
      "outputs": [],
      "source": [
        "pred = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr2ZmjbrsJvJ"
      },
      "outputs": [],
      "source": [
        "result_test = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHx8qtiEsMlc"
      },
      "outputs": [],
      "source": [
        "result_test.r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6oefK5QsAKk"
      },
      "outputs": [],
      "source": [
        "pred.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ0NdeI6ziXs"
      },
      "outputs": [],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsVcfyDu0wST"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n",
        "# metadata\n",
        "print(adult.metadata)\n",
        "\n",
        "# variable information\n",
        "print(adult.variables)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TPTtb4L0zjN"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkkB2g270-SG"
      },
      "outputs": [],
      "source": [
        "data = spark.read.csv('/content/adult.data')\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oof9exA71R_C"
      },
      "outputs": [],
      "source": [
        "labels = ['age', 'workclass', 'fnlwgt', 'education', 'numbers', 'marital', 'occupation', 'relation', 'race', 'gender', 'gain', 'loss', 'hourlypay', 'country', 'income']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbhypvOs1aSI"
      },
      "outputs": [],
      "source": [
        "df = data.toDF(*labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO5ciVOY16Le"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G32Vnwf1huJ"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vytWWq5a2ulK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "new_df = df.withColumn('age' , col('age').cast('integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofYNvOyu24I9"
      },
      "outputs": [],
      "source": [
        "new_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgTWp9cX2DxC"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in [ 'fnlwgt' , 'numbers', 'gain', 'loss', 'hourlypay']:\n",
        "  new_df = new_df.withColumn(i , col(i).cast('integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okGWhH2C3HF5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "new_df.select([count(when(col(c).isNull(), c )).alias(c) for c in new_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue7sgN2E3pIZ"
      },
      "outputs": [],
      "source": [
        "df.select('workclass').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcy3Sgjs3xfs"
      },
      "outputs": [],
      "source": [
        "df = new_df.replace(\" ?\" , None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-5Lb5NQ39KI"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df.select([count(when(col(c).isNull(), c )).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28WqdlHT4BzP"
      },
      "outputs": [],
      "source": [
        "df.groupby('occupation').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4R8KTrn4OCW"
      },
      "outputs": [],
      "source": [
        "df = df.fillna(\" United-States\", subset = ['country'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f09zNglh4cey"
      },
      "outputs": [],
      "source": [
        "df = df.fillna(\" Private\", subset = ['workclass'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l0x-ZHw4eDb"
      },
      "outputs": [],
      "source": [
        "df = df.fillna(\" Prof-specialty\", subset = ['occupation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEhyYUBv4v8l"
      },
      "outputs": [],
      "source": [
        "df.select([count(when(col(c).isNull(), c )).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ein8UhxE40L2"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer , VectorAssembler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brF792pG5Y26"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VksLIC7W5SA5"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [ 'workclass',\n",
        " 'education',\n",
        " 'marital',\n",
        " 'occupation',\n",
        " 'relation',\n",
        " 'race',\n",
        " 'gender',\n",
        " 'country']\n",
        "numeric_cols = [ 'age' ,'fnlwgt' , 'numbers', 'gain', 'loss', 'hourlypay']\n",
        "label = 'income'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXWuQsdU5x0S"
      },
      "outputs": [],
      "source": [
        "indexer = [StringIndexer(inputCol = c , outputCol = f\"{c}_index\", handleInvalid = 'keep') for c in categorical_cols]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34Mzvaf37cd3"
      },
      "outputs": [],
      "source": [
        "label_indexer = StringIndexer(inputCol = 'income', outputCol = 'label', handleInvalid = 'keep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FPiqXFK7hpW"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols = [f'{c}_index' for c in categorical_cols] + numeric_cols, outputCol = 'features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7rUL-aA8Gxw"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKYvYdU28ex0"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages = indexer + [assembler , label_indexer , lr] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHSYg8vs85zI"
      },
      "outputs": [],
      "source": [
        "train_data , test_data = df.randomSplit([.8, .2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-K71fk99AXe"
      },
      "outputs": [],
      "source": [
        "model = pipeline.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEJsEE6q9IsK"
      },
      "outputs": [],
      "source": [
        "prediction = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBQQTT3q9Sx5"
      },
      "outputs": [],
      "source": [
        "prediction.select('label', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX1iWnHR9-p5"
      },
      "outputs": [],
      "source": [
        "prediction.groupby('label', 'prediction').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwuugKNz9zaX"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol = 'prediction', labelCol = 'label', metricName = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Tb2H-2-cKO"
      },
      "outputs": [],
      "source": [
        "evaluator.evaluate(prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}